{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do I create a _batch_ of tasks and _dynamically_ choose a workflow for each of my files?\n",
    "### Overview\n",
    "_[Use Case] Batch process a set of WXS files with SAMTools at specific regions of interest_\n",
    "\n",
    "This example will process Whole Exome Sequencing (WXS) files (bam) and thier indices (bai) and use the Seven Bridges version of [SAM Tools](https://github.com/samtools/samtools) software suite to extract a _sbset of alignments_. Conceptually the API will scan all available files within a project; for each _bam_ file it finds the corresponding:\n",
    " - index file (bai)\n",
    " - _Case\\_UUID_\n",
    " - _Disease Name_\n",
    " - _File Size_\n",
    "\n",
    "The bam _File Size_ will determine the EC2 allocation size<sup>1</sup> for SAMTools, _Case_UUID_ and _Disease Name_ are also collected to illustrate the technique. Users can build on this for refining the batch. The script then loops the bam files and starts one task for each. There is also the option to ping the CGC for task completion and download files. Results are always saved on the CGC\n",
    "\n",
    "We will work through everything else. This example requires a mix of skills. \n",
    " \n",
    " - Data Browser Use\n",
    " - Workflow Copying\n",
    " - API use\n",
    "\n",
    "<sup>1</sup> Note: dynamic allocation size is already available on the back-end; we don't **need** to do this manually via API. However, we intend to demonstrate the flexibility you can achieve for your _own ends_.\n",
    "\n",
    "### Prerequisites\n",
    " 1. You need your _authentication token_ and the API needs to know about it. See <a href=\"set_AUTH_TOKEN.ipynb\">**set_AUTH_TOKEN.ipynb**</a> for details.\n",
    " 2. You need _TCGA **Controlled** Data_ access\n",
    " \n",
    "### WARNING\n",
    "This will burn through some processing credits, depending on how many files you pull from _Data Browser_ (about $0.12 to $0.23 per file). You can create _DRAFT_ tasks to just see how it works, swap the commenting in **Build and run tasks** to only run: \n",
    "```python\n",
    "myTask = API(method='POST', data=new_task, path='tasks/')        # task created in DRAFT state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps on the GUI\n",
    "  \n",
    "We will always be working with the [Cancer Genomics Cloud](https://cgc.sbgenomics.com), but will mix _GUI_ and _API_ tasks here. GUI tasks will be descriptive (markdown cells); API tasks will be in an executable cell but preceded with an explanation in a markdown cell. \n",
    "\n",
    "### 1) Create a project\n",
    "Create a project in the GUI. Name it 'Batch is Super'. Mark that it will contain _TCGA Controlled Data_\n",
    "<img src=\"images/batch_1.png\"> \n",
    "\n",
    "### 2) Use _Data Browser_ to get expressions\n",
    "Go into _Data Browser_ and construct the following query (should generate approximately 132 files). Click the DataFormat nodes and **Copy files to Project**. Select _Batch is Super_.\n",
    "<img src=\"images/batch_2.png\"> \n",
    "\n",
    "    - hasCase\n",
    "         - hasAgeAtDiagnosis = 60 < age < 65\n",
    "         - hasDiseaseType = Acute Myeloid Leukemia\n",
    "         - hasGender = MALE\n",
    "         - hasFile\n",
    "             - hasDataFormat = BAM, BAI\n",
    "             - hasExperimentalStrategy = WXS\n",
    "         \n",
    "Now lets get into some iPython!\n",
    "\n",
    "## Imports and Definitions\n",
    "We will use a Python class (API) as a wrapper for API calls. All classes and methods defined in <a href=\"defs/apimethods.py\" target=\"_blank\">_defs/apimethods.py_</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from defs.apimethods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Workflow Language (CWL) description of our Apps\n",
    "We could copy these Apps from the Public Reference Apps, but _what fun would that be?_ Let's instead specify the exact CWL JSON! In the **Find your project** section, we will create our apps with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('files/reg_tool.json', 'r')\n",
    "regular_tool_raw = f.read()\n",
    "regular_tool = json.loads(regular_tool_raw)\n",
    "\n",
    "f = open('files/large_tool.json', 'r')\n",
    "large_tool_raw = f.read()\n",
    "large_tool = json.loads(large_tool_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input\n",
    "We need to set a few things here, depending on which areas we want to look at. Additionally, this would be the space to set project and tool names.\n",
    "\n",
    "Please be careful that target_region **must be an array** even if there is only one target. If you wanted to only target a single SNP on chromosome 2 at base pair 387840, use:\n",
    "```python\n",
    "target_region = ['2:387840-387840']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set project and app names:\n",
    "project_name = 'Batch is Super'\n",
    "app_name_reg = 'SAMtools View Region (Regular)' \n",
    "app_name_XL = 'SAMtools View Region (Large)' \n",
    "\n",
    "# Limit for calling app_name_reg, if (size >= size_limit) call app_name_XL\n",
    "size_limit = 25000000000                        \n",
    "\n",
    "# File extensions we will be working with\n",
    "input_ext = 'bam'\n",
    "index_ext = 'bai'\n",
    "output_ext = 'sam'\n",
    "\n",
    "# Prefix for created tasks\n",
    "task_name = 'batch_SAMtoolsView_'\n",
    "\n",
    "# Regions to investigate (item[1] is TP53)\n",
    "target_region = ['2:387840-387840',\n",
    "                 '17:7668402-7687550',\n",
    "                 '3:192000-192000']         # format is N:n-m  where N is chromosome #,\n",
    "                                            #   n is starting base pair, and m is ending base pair. if n = m, it is a\n",
    "                                            #   SNP. Can be an array of regions. If a single SNP, keep it an array of 1\n",
    "# n_regions = len(target_region)\n",
    "# if n_regions > 1:\n",
    "#     flag = {'multi_region': True}\n",
    "# else:\n",
    "#     flag = {'multi_region': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find your project\n",
    "This code searches through all projects in your account and then gets the details of the _project\\_name_ to make sure you've properly set the things in the GUI above:\n",
    "\n",
    " - All files\n",
    " \n",
    "Then we will _create_ the apps we need from the CWL in the earlier cell\n",
    "\n",
    " - All apps\n",
    "     - details of the app matching app_name_reg\n",
    "     - details of the app matching app_name_XL\n",
    "     \n",
    "#### PROTIPS\n",
    "* The recipes involved in this cell are [here](../../Recipes/CGC/projects_detailOne.ipynb), [here](../../Recipes/CGC/files_listAll.ipynb), and [here](../../Recipes/CGC/files_detailOne.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LIST all projects\n",
    "existing_projects = API(path='projects')  \n",
    "\n",
    "# DETAIL my_project\n",
    "p_index = existing_projects.name.index(project_name)\n",
    "my_project = API(path=('projects/'+ existing_projects.id[p_index]))  \n",
    "\n",
    "# LIST all files in project\n",
    "my_files = API(path='files', query={'limit':100, 'project': my_project.id}) \n",
    "\n",
    "# CREATE your tools from the JSON\n",
    "API(path=('apps/' + my_project.id + '/sam-normal/0/raw'), \\\n",
    "             method='POST', data = regular_tool)\n",
    "API(path=('apps/' + my_project.id + '/sam-large/0/raw'), \\\n",
    "             method='POST', data = large_tool)\n",
    "\n",
    "# LIST all apps in project and make sure they were created\n",
    "my_apps = API(path='apps', query={'limit':100, 'project': my_project.id}) \n",
    "if len(my_apps.id) > 1:\n",
    "    for ii, a_name in enumerate(my_apps.name):\n",
    "        if app_name_reg == a_name:\n",
    "            my_inputs = API(path=('apps/' + my_apps.id[ii]))\n",
    "        elif app_name_XL == a_name:\n",
    "            my_inputs_XL = API(path=('apps/' + my_apps.id[ii]))\n",
    "    del ii, a_name\n",
    "else:\n",
    "    print \"Apps were not created in selected project, cannot continue\"\n",
    "    raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize files into a **cohort**\n",
    "The _Data Browser_ is excellent for finding files. However, there are challenges to working with them smoothly, especially as the number of files grows. Specifically\n",
    "\n",
    " - File naming ambiguity between patients and centers (related to the change from **TCGA Barcode** to **UUID**)\n",
    "     - This is not a critical issue here, but as an example we save mulitple metadata before starting tasks\n",
    "         - CASE_UUID\n",
    "         - disease_type\n",
    "         - size (of file)\n",
    " - Uncertainty whether samples are matched (e.g. does the index file (BAI) exist for all input files (BAM))\n",
    "     - check comment = index file exists\n",
    "     \n",
    "We only take action on file size, calling a task with a bigger EC2 allocation if the input file exceeds the standard. However, this example should be _illustrative_ for users needing other metadata which is all accessible from \n",
    "``` python \n",
    "singleFile['metadata']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case_ids = {'uuid': [None], 'index': [None], 'input': [None], 'size': [None], 'disease': [None]}\n",
    "\n",
    "# Collect input file metadata. Saving the case_UUID and DiseaseType \n",
    "# (not used, but you can add whatever filtering rocks your world)\n",
    "for ii, f_name in enumerate(my_files.name):\n",
    "    if f_name[-len(input_ext):] == input_ext:      # input_ext defined for 'SAMtools View Region'\n",
    "        single_file = API(path=('files/' + my_files.id[ii]))\n",
    "        if (f_name + '.' + index_ext) in my_files.name:   # INDEX exists for this INPUT\n",
    "            case_ids['uuid'].append(single_file.metadata['case_uuid'])\n",
    "            case_ids['input'].append(ii)\n",
    "            case_ids['index'].append(my_files.name.index(f_name + '.' + index_ext))\n",
    "            case_ids['size'].append(single_file.size)\n",
    "            case_ids['disease'].append(single_file.metadata['disease_type'])\n",
    "\n",
    "case_ids['uuid'].pop(0)\n",
    "case_ids['index'].pop(0)\n",
    "case_ids['input'].pop(0)\n",
    "case_ids['size'].pop(0)\n",
    "case_ids['disease'].pop(0)\n",
    "\n",
    "print('There are %i indexed bam files within the project' % (len(case_ids['uuid'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run tasks\n",
    "Here we use the API to create a _new\\_task_ dictionary that we will use for each pair of files<sup>1</sup>. Once it connects to the CGC, we will have all of the front-end tasks drafted and starting within seconds.\n",
    "\n",
    "<sup>1</sup> Note, we overwrite the 'app' entry after creating the task to switch to a larger EC2 instance based on \n",
    "     \n",
    "#### PROTIPS\n",
    "* Detailed documentation of this particular REST architectural style request is available [here](http://docs.cancergenomicscloud.org/docs/create-a-new-task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_task_list = [None]\n",
    "for ii,case_id in enumerate(case_ids['input']):    \n",
    "    new_task = {\n",
    "        'description': 'Created from batch_SAMtoolsView.ipynb',\n",
    "        'name': (task_name + str(ii)),\n",
    "        'app': my_inputs.id,\n",
    "        'project': my_project.id,\n",
    "        'inputs': {\n",
    "            'regions_array': target_region,       # region(s) of interest\n",
    "            'input_bam_or_sam_file': {                 # BAM file\n",
    "                'class': 'File',\n",
    "                'path': my_files.id[case_id],\n",
    "                'name': my_files.name[case_id]\n",
    "            },\n",
    "            'input_index': {                           # BAI index\n",
    "                'class': 'File',\n",
    "                'path': my_files.id[case_ids['index'][ii]],\n",
    "                'name': my_files.name[case_ids['index'][ii]]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "#         if flag['multi_region']:\n",
    "#             for reg in target_region[1:]:\n",
    "#                 new_task['inputs']['regions_array'].append(reg)\n",
    "\n",
    "    # check if larger task is need\n",
    "    if case_ids['size'][ii] >= size_limit:\n",
    "        new_task['app'] = my_inputs_XL.id\n",
    "\n",
    "    # CREATE and RUN tasks\n",
    "    my_task = API(method='POST', data=new_task, path='tasks/', query = {'action': 'run'})\n",
    "    my_task_list.append(my_task.id)\n",
    "    # ALTERNATIVE: create a DRAFT tasks, do not run\n",
    "#     myTask = API(method='POST', data=new_task, path='tasks/')        # task created in DRAFT state\n",
    "my_task_list.pop(0)\n",
    "\n",
    "print(\"\"\"\n",
    "%i tasks have been created. Enjoy a break, treat yourself to a coffee, \n",
    "and come back to us once you've gotten an email that tasks are done.\n",
    "(alternatively, use the task monitoring cells below)\"\"\" % (len(my_task_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check task completion\n",
    "These tasks may take a long time to complete, here are two ways to check in on them:\n",
    "* Wait for email confirmation <sup>1</sup>\n",
    "* Ping the task to see it's _status_. Here we use a 2 min interval, adjust it appropriately for longer or shorter workflows\n",
    "\n",
    "<sup>1</sup> Emails will arrive regardless of whether the task was started by GUI or API\n",
    "\n",
    "#### PROTIPS\n",
    "* The closest recipe for _monitoring tasks_ is [here](../../Recipes/CGC/tasks_monitorAndGetResults.ipynb)\n",
    "* Detailed documentation of this particular REST architectural style request is available [here](http://docs.cancergenomicscloud.org/docs/perform-an-action-on-a-specific-task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [USER INPUT] Set loop time (seconds):\n",
    "loop_time = 120\n",
    "\n",
    "for t_id in my_task_list:\n",
    "    # Check on one task at a time, \n",
    "    #  if ANY running, we are not done (no sense to query others)\n",
    "    flag = {'taskRunning': True}\n",
    "    while flag['taskRunning']:\n",
    "        task = api_call(('tasks/' + t_id))\n",
    "        if task['status'] == 'COMPLETED':\n",
    "            flag['taskRunning'] = False\n",
    "            print('Task has completed, life is beautiful')\n",
    "        elif (task['status'] == 'FAILED') or (task['status'] == 'ABORTED'):\n",
    "            print('Task (%s) failed, check it out' \\\n",
    "                  % (t_id))\n",
    "            flag['taskRunning'] = False\n",
    "        else:\n",
    "            sleep(loop_time) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Branch point\n",
    "You have now completed all (most) of your tasks and will have a large set of output files. One interesting case would be another set of code here to do\n",
    " - Quality Control\n",
    " - Second level of analysis (e.g. these output files will serve as inputs to another App, e.g see [thyroid.ipynb](thyroid.ipynb)\n",
    "\n",
    "## (optional) Download processed SAM files\n",
    "You will already have all of these saved in your project (and a _lot of emails_ - one for each completed task). You may also download all of the SAMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [USER INPUT] Set file extension(s) to download here:\n",
    "\n",
    "dl_list = [\"links to file downloads\"]\n",
    "\n",
    "my_files = API(path='files', query={'project': my_project.id})\n",
    "for ii, f_name in enumerate(my_files.name):\n",
    "    if (f_name[-len(output_ext):] == output_ext):\n",
    "        dl_list.append(api_call(path=('files/' + my_files.id[ii] + '/download_info'))['url'])\n",
    "        \n",
    "download_files(dl_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
