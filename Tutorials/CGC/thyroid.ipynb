{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do I _parallelize_ and _split_ a workflow?\n",
    "\n",
    "[Use Case] Collect paired RNA expressions, differentiate normal and tumor\n",
    "\n",
    "This example will process RNA-seq expression files (bams) and use the [Cufflinks](http://cole-trapnell-lab.github.io/cufflinks/cuffcompare/) software suite to calculate _differential expression_. Conceptually the API will scan all available files, matches _tumor_ and _normal_ samples, adds the reference files, and then start two rounds of _tasks_.\n",
    "\n",
    "Tasks are broken<sup>1</sup> into a *first\\_stage* (_N_ tasks accepting a pair of inputs {Normal,Tumor}) and single *second\\_stage* (accepting _N_ x 2 abundances files). After creating the _N_ front-end tasks, the app **waits** for completion, pinging the CGC every 10 minutes. Once all front-end tasks are completed, the outputs are collected and used to create a single back-end task. Results are saved on the CGC\n",
    " \n",
    "This example requires a mix of skills. \n",
    "\n",
    " - Case Explorer Use \n",
    " - Data Browser Use\n",
    " - Workflow Modification\n",
    " \n",
    "<sup>1</sup> Note: dynamic batching and splitting is already available on the back-end; we don't **need** to do this manually via API. However, we intend to demonstrate the flexibility you can achieve for your _own ends_.\n",
    " \n",
    "### Prerequisites\n",
    " 1. You need your _authentication token_ and the API needs to know about it. See <a href=\"set_AUTH_TOKEN.ipynb\">**set_AUTH_TOKEN.ipynb**</a> for details.\n",
    " 2. You need _TCGA **Controlled** Data_ access\n",
    " \n",
    "  \n",
    "### WARNING\n",
    "This will burn through **considerable** processing credits, depending on how many files you pull from _Data Browser_ (about \\$2-4 per sample pair plus \\$3-5 for the second stage). You can create _DRAFT_ tasks to just see how it works, swap the commenting in **Build and run tasks** to only run: \n",
    "```python\n",
    "myTask = API(method='POST', data=new_task, path='tasks/')        # task created in DRAFT state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps on the GUI\n",
    "  \n",
    "We will always be working with the [Cancer Genomics Cloud](https://cgc.sbgenomics.com), but will mix _GUI_ and _API_ tasks here. GUI tasks will be descriptive (markdown cells); API tasks will be in an executable cell but preceded with an explanation in a markdown cell. \n",
    "\n",
    "### 1) Create a project\n",
    "Create a project in the GUI. Name it 'Divide and Conquer'. Mark that it will contain _TCGA Controlled Data_\n",
    "\n",
    "<img src=\"images/thyroid_1.png\"> \n",
    "\n",
    "### 2) Screen cases using _Case Explorer_\n",
    " - In _Case Explorer_ select Thyroid Cancer (THCA) and click on the BRAF gene\n",
    " - In the bottom left, unclick \"No\" so only cases with a BRAF mutation are included\n",
    " - drag a box around all data points\n",
    " - click the green _Continue To Data Browser_ button at bottom left\n",
    "<img src=\"images/thyroid_2.png\" height=\"780\" width=\"542\"> \n",
    "\n",
    "### 3) Use _Data Browser_ to get expressions\n",
    " - In _Data Browser_ add some nodes to restrict the files.\n",
    "     - hasSample\n",
    "         - hasSampleType = {Primary Tumor}\n",
    "         - hasFile\n",
    "             - hasExperimentalStrategy = RNA-seq\n",
    "             - hasDataFormat = BAM\n",
    "      - hasSample\n",
    "         - hasSampleType = {Solid Tissue Normal}\n",
    "         - hasFile\n",
    "             - hasExperimentalStrategy = RNA-seq\n",
    "             - hasDataFormat = BAM\n",
    "<img src=\"images/thyroid_3.png\" height=\"780\" width=\"497\"> \n",
    "\n",
    "Export **each branch** of the tree to the project 'Divide and Conquer', each one should add 29 files to the project.\n",
    "\n",
    "### 3) Add tools to your project\n",
    "From the _Apps_ tab of the project, click on **Add app**.\n",
    "<img src=\"images/thyroid_4.png\"> \n",
    "\n",
    "Click **Browse Public Apps**, then search for _RNA-Seq Differential Expression_\n",
    "<img src=\"images/thyroid_5.png\"> \n",
    "\n",
    "Click **Copy**, select your project, and rename the app to **rna-seq-diff-expression-first** and add it to your project\n",
    "<img src=\"images/thyroid_6.png\">\n",
    "\n",
    "Click **Copy** _again_, select your project, and rename the app to **rna-seq-diff-expression-second** and add it to your project\n",
    "<img src=\"images/thyroid_7.png\"> \n",
    "\n",
    "### 4) Sanity Check\n",
    "If all has gone well, the _Apps_ panel in the 'Divide and Conquer' project should look like this:\n",
    "<img src=\"images/thyroid_8.png\" height=\"780\" width=\"162\">\n",
    "Notice they have the same name, we are going to fix that!\n",
    "\n",
    "### 5) Hack apart the tools\n",
    "Open the top app, it should be the _second_ version (you can see this in the bottom right). Go to **Edit** to modify this tool. Below you see the original **RNA-seq Differential Expression** workflow. Notice that there is a _Cuffquant_ tool that deals with **single pairs of inputs** and a _Cuffdiff_ that deals with **all** of the abundances. We are going to break this apart.\n",
    "<img src=\"images/thyroid_9.png\" height=\"780\" width=\"342\"> \n",
    "\n",
    "In the Tool Editor window, click on ADDITIONAL INFO and change the **Label** to _second RNA-seq cuffdiff_. Click Save to close that window. Then delete nodes until your screen looks like this:\n",
    "<img src=\"images/thyroid_10.png\" height=\"780\" width=\"349\"> \n",
    "\n",
    "Click and drag from the second input to Cuffdiff to the left screen edge. This will create an _input node_ named 'sample_files'. \n",
    "<img src=\"images/thyroid_11.png\" height=\"780\" width=\"281\"> \n",
    "\n",
    "Now we need to _unlock_ some of the configuration parameters so we can modify them at run-time (i.e. they will be task **inputs**). Click on the _Cuffdiff_ node and in the right panel (_PARAMS_ tab) scroll-down to _min\\_reps\\_for\\_js\\_test_ and unlock it (click the lock icon so it changes to \"open\" and is colored blue) as shown here:\n",
    "<img src=\"images/thyroid_11a.png\" height=\"780\" width=\"361\"> \n",
    "\n",
    "Repeat the same unlocking (all on the _Cuffdiff_ node) for\n",
    "* FDR\n",
    "* library_type\n",
    "* dispersion_method\n",
    "* library_norm_method\n",
    "\n",
    "Finally, click Save to update the workflow.\n",
    "\n",
    "Now we need to make the other half. Go to the Apps panel. Click on the workflow still named **RNA-seq Differential Expression** and then click **Edit**. In the Tool Editor window, click on ADDITIONAL INFO and change the **Label** to _first RNA-seq cuffquant_. Click Save to close that window. Then delete nodes until your screen looks like this:\n",
    "<img src=\"images/thyroid_12.png\"> \n",
    "\n",
    "Click and drag from the output of Cuffquant to the right screen edge. This will create an _output node_ named 'abundances'. \n",
    "<img src=\"images/thyroid_13.png\" height=\"780\" width=\"284\"> \n",
    "\n",
    "A few final issues, the Group input names are wrong here and need to be changed. Click on the top _SBG\\_Group\\_Input_, then on the far right click the lock icon so this field is _Exposed_. \n",
    "<img src=\"images/thyroid_14.png\" height=\"780\" width=\"263\"> \n",
    "\n",
    "We will be able to change the name at run time. Do the same for the bottom _SBG\\_Group\\_Input_. \n",
    "<img src=\"images/thyroid_15.png\"> \n",
    "\n",
    "Click on the _Cuffquant_ node and in the right panel scroll down. Unlock the _library\\_type_ input\n",
    "<img src=\"images/thyroid_16.png\" height=\"780\" width=\"395\"> \n",
    "\n",
    "Let's save some money (at the _expense_ of time), by changing the recommended instance for the workflow. First click the gear icon on the top right, and click _Settings_\n",
    "\n",
    "<img src=\"images/thyroid_17.png\"> \n",
    "\n",
    "Then change the **sbg:AWSInstanceYype** to **c3.2xlarge**\n",
    "<img src=\"images/thyroid_18.png\" height=\"780\" width=\"411\"> \n",
    "\n",
    "Finally, click Save to update the workflow.\n",
    "\n",
    "### 6) Sanity Check\n",
    "If all has gone well, your Apps panel should have two workflows:\n",
    "\n",
    " - first RNA-seq cuffquant\n",
    " - second RNA-seq cuffdiff\n",
    "     \n",
    "Now lets get into some iPython!\n",
    "\n",
    "## Imports and Definitions\n",
    "We will use a Python class (API) as a wrapper for API calls. All classes and methods defined in <a href=\"defs/apimethods.py\" target=\"_blank\">_defs/apimethods.py_</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from defs.apimethods import *\n",
    "import numpy as np\n",
    "\n",
    "# extra method for organizing the files by metadata\n",
    "def my_zip_sort(indexList, otherList):\n",
    "    srt = np.argsort(indexList)\n",
    "    return [otherList[i] for i in srt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input\n",
    "We need to set a few things here, depending on which areas we want to look at. Additionally, this would be the space to set project and tool names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set project and app names:\n",
    "project_name = 'Divide and Conquer'\n",
    "app_name_first = 'first RNA-seq cuffquant' \n",
    "app_name_second = 'second RNA-seq cuffdiff'                    \n",
    "\n",
    "# File extensions we will be working with\n",
    "input_ext = 'bam'\n",
    "annote_ext = 'gtf'\n",
    "ref_ext = 'fasta'\n",
    "abund_ext = 'cxb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find your project\n",
    "This code searches through all projects in your account and then gets the details of the _project\\_name_ to make sure you've properly set the things in the GUI above:\n",
    "\n",
    " - All files\n",
    " - All apps\n",
    "     - details of the app matching app_name_first \n",
    "     - details of the app matching app_name_second\n",
    "     \n",
    "#### PROTIPS\n",
    "* The recipes involved in this cell are [here](../../Recipes/CGC/projects_detailOne.ipynb), [here](../../Recipes/CGC/files_listAll.ipynb), and [here](../../Recipes/CGC/files_detailOne.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LIST all projects\n",
    "existing_projects = API(path='projects')  \n",
    "\n",
    "# DETAIL my_project\n",
    "p_index = existing_projects.name.index(project_name)\n",
    "my_project = API(path=('projects/'+ existing_projects.id[p_index]))  \n",
    "\n",
    "# LIST all files in project\n",
    "my_files = API(path='files', query={'limit':100, 'project': my_project.id}) \n",
    "\n",
    "# LIST all apps in project and make sure they were created\n",
    "my_apps = API(path='apps', query={'limit':100, 'project': my_project.id}) \n",
    "if len(my_apps.id) > 1:\n",
    "    for ii, a_name in enumerate(my_apps.name):\n",
    "        if app_name_first == a_name:\n",
    "            my_inputs_first = API(path=('apps/' + my_apps.id[ii]))\n",
    "        elif app_name_second == a_name:\n",
    "            my_inputs_second = API(path=('apps/' + my_apps.id[ii]))\n",
    "    del ii, a_name\n",
    "else:\n",
    "    print \"Apps were not created in selected project, cannot continue\"\n",
    "    raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add reference files\n",
    "We need our **reference** and **annotation** files, which we will take from the Public Reference files\n",
    "\n",
    "#### PROTIPS\n",
    "* The recipe involved in this cell are [here](../../Recipes/CGC/files_copyFromPublicReference.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [USER INPUT] Set project and file names:\n",
    "p_name = 'admin/sbg-public-data'\n",
    "f_list = ['ucsc.hg19.fasta', 'human_hg19_genes_2014.gtf']                \n",
    "\n",
    "# LIST all files in the source and target project\n",
    "my_files_source = API(path='files', \\\n",
    "                      query={'project':p_name, 'limit':100})\n",
    "my_files_target = API(path='files', \\\n",
    "                      query={'project': my_project.id})\n",
    "\n",
    "for f_name in f_list:\n",
    "    f_index = my_files_source.name.index(f_name)\n",
    "    if f_name not in my_files_target.name:\n",
    "        print('File (%s) does not exist in Project (%s); copying now' % \\\n",
    "              (f_name, my_project.id))\n",
    "\n",
    "        # COPY the selected file from source to target project\n",
    "        API(path=('files/' + my_files_source.id[f_index] + '/actions/copy'), \\\n",
    "            method='POST', \\\n",
    "            data={'project': my_project.id,\\\n",
    "                  'name': f_name}) \n",
    "\n",
    "        # re-list files in target project to verify the copy worked\n",
    "        my_files_target = API(path='files', \\\n",
    "                              query={'project': my_project.id})\n",
    "\n",
    "        if f_name in my_files_target.name:\n",
    "            print('Sucessfully copied one file!')\n",
    "        else:\n",
    "            print('Something went wrong...')\n",
    "            \n",
    "# We are done copying files, let's clean up a little\n",
    "del my_files_source, my_files_target\n",
    "my_files = API(path='files', query={'project': my_project.id})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize files into a **cohort**\n",
    "The _Data Browser_ is excellent for finding files. However, there are challenges to working with them smoothly, especially as the number of files grows. Specifically\n",
    "\n",
    " - File naming ambiguity between patients and centers (related to the change from **TCGA Barcode** to **UUID**)\n",
    "     - This is not a critical issue here, but as an example we save mulitple metadata before starting tasks\n",
    "         - CASE_UUID\n",
    "         - disease_type\n",
    "         - size (of file)\n",
    " - Uncertainty whether samples are matched (e.g. does the index file (BAI) exist for all input files (BAM))\n",
    "     - clean any unmatched UUIDs\n",
    "     - sort CASE_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gtf_ind = None\n",
    "fasta_ind = None\n",
    "case_ids = {'uuid': [None], 'index': [None], 'type': [None]}\n",
    "\n",
    "# Collect input file metadata. Saving the case_UUID and SampleType \n",
    "for ii, f_name in enumerate(my_files.name):\n",
    "    if f_name[-len(input_ext):] == input_ext:\n",
    "        single_file = API(path=('files/' + my_files.id[ii]))\n",
    "        case_ids['uuid'].append(single_file.metadata['case_uuid'])\n",
    "        case_ids['index'].append(ii)\n",
    "        if single_file.metadata['sample_type'] == 'Solid Tissue Normal':\n",
    "            case_ids['type'].append('Normal')\n",
    "        else:\n",
    "            case_ids['type'].append('Tumor')\n",
    "    elif f_name[-len(annote_ext):] == annote_ext:\n",
    "        gtf_ind = ii\n",
    "    elif f_name[-len(ref_ext):] == ref_ext:\n",
    "        fasta_ind  = ii\n",
    "        \n",
    "case_ids['uuid'].pop(0)\n",
    "case_ids['index'].pop(0)\n",
    "case_ids['type'].pop(0)\n",
    "\n",
    "# sort CASE_IDs\n",
    "case_ids['type'] = my_zip_sort(case_ids['uuid'], case_ids['type'])\n",
    "case_ids['index'] = my_zip_sort(case_ids['uuid'], case_ids['index'])\n",
    "case_ids['uuid'] = my_zip_sort(case_ids['uuid'], case_ids['uuid'])\n",
    "\n",
    "# clean any unmatched UUIDs\n",
    "to_delete = [None]\n",
    "ii = 0\n",
    "while ii<len(case_ids['uuid']):\n",
    "    if ii==len(case_ids['uuid'])-1:\n",
    "        if case_ids['uuid'][ii] != case_ids['uuid'][ii-1]:\n",
    "            to_delete.append(ii)\n",
    "            ii += 1\n",
    "    else:\n",
    "        if case_ids['uuid'][ii] == case_ids['uuid'][ii+1]:\n",
    "            ii += 2\n",
    "        else:\n",
    "            to_delete.append(ii)\n",
    "            ii += 1\n",
    "to_delete.pop(0)\n",
    "\n",
    "for ii in np.sort(to_delete)[::-1]:\n",
    "    case_ids['index'].pop(ii)\n",
    "    case_ids['type'].pop(ii)\n",
    "    case_ids['uuid'].pop(ii)\n",
    "    \n",
    "del to_delete, ii\n",
    "\n",
    "print('We have %i matched Tumor-Normal pairs in this project' % (len(case_ids['index'])/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run _first-stage_ tasks\n",
    "Here we use the API to create a _new\\_task_ dictionary that we will use for each pair of files. Once it connects to the CGC, we will have all of the front-end tasks drafted and starting within seconds.\n",
    "\n",
    "#### PROTIPS\n",
    "* Detailed documentation of this particular REST architectural style request is available [here](http://docs.cancergenomicscloud.org/docs/create-a-new-task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Format the JSON to pass values to the FRONT-END workflow (frontend RNA-seq cuffquant)\n",
    "my_task_list = [None]\n",
    "for ii in range(0,len(case_ids['uuid'])/2):\n",
    "    new_task = {'description': 'Quantify RNA expression using cuffquant (pair-wise). created with thyroid.ipynb',\n",
    "        'name': ('first_stage_%i' % ii),\n",
    "        'app': my_inputs_first.id,                                         \n",
    "        'project': my_project.id,\n",
    "        'inputs': {\n",
    "            'Reference':{                                             \n",
    "                'class':'File',\n",
    "                'path': my_files.id[fasta_ind],\n",
    "                'name': my_files.name[fasta_ind]\n",
    "            },\n",
    "            'Annotations': {                                          \n",
    "                'class': 'File',\n",
    "                'path': my_files.id[gtf_ind],\n",
    "                'name': my_files.name[gtf_ind]\n",
    "            },\n",
    "            # Define App Settings (these are the things we \"unlocked\")\n",
    "            'group_name': 'Normal',\n",
    "            'group_name_1': 'Tumor',\n",
    "            'library_type': u'ff-unstranded'\n",
    "        }\n",
    "    }\n",
    "    for jj in range(0,2):\n",
    "        file_desc = {'class': 'File',\n",
    "                    'path': my_files.id[case_ids['index'][(ii-1)*2 + jj]],\n",
    "                    'name': my_files.name[case_ids['index'][(ii-1)*2 + jj]]\n",
    "        }\n",
    "        if case_ids['type'][(ii-1)*2 + jj] == 'Tumor':\n",
    "            new_task['inputs']['Group_ERR315335'] = [file_desc]         # this is an ARRAY input, so must be a list\n",
    "        else:\n",
    "            new_task['inputs']['Group_ERR315421'] = [file_desc]\n",
    "        \n",
    "    my_task = API(method='POST', data=new_task, path='tasks/', query = {'action': 'run'})\n",
    "    my_task_list.append(my_task.id)\n",
    "my_task_list.pop(0)\n",
    " \n",
    "print(\"\"\"\n",
    "%i tasks have been created. Enjoy a break, treat yourself to a muffin, \n",
    "and come back to us once you've gotten an email that tasks are done.\n",
    "(alternatively, use the task monitoring cells below)\"\"\" % (len(my_task_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check task completion\n",
    "These tasks may take a long time to complete, here are two ways to check in on them:\n",
    "* Wait for email confirmation <sup>1</sup>\n",
    "* Ping the task to see it's _status_. Here we use a 10 min interval, adjust it appropriately for longer or shorter workflows\n",
    "\n",
    "<sup>1</sup> Emails will arrive regardless of whether the task was started by GUI or API\n",
    "\n",
    "#### PROTIPS\n",
    "* The closest recipe for _monitoring tasks_ is [here](../../Recipes/CGC/tasks_monitorAndGetResults.ipynb)\n",
    "* Detailed documentation of this particular REST architectural style request is available [here](http://docs.cancergenomicscloud.org/docs/perform-an-action-on-a-specific-task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [USER INPUT] Set loop time (seconds):\n",
    "loop_time = 600\n",
    "\n",
    "for t_id in my_task_list:\n",
    "    # Check on one task at a time, \n",
    "    #  if ANY running, we are not done (no sense to query others)\n",
    "    flag = {'taskRunning': True}\n",
    "    while flag['taskRunning']:\n",
    "        task = api_call(('tasks/' + t_id))\n",
    "        if task['status'] == 'COMPLETED':\n",
    "            flag['taskRunning'] = False\n",
    "            print('Task has completed, life is beautiful')\n",
    "        elif (task['status'] == 'FAILED') or (task['status'] == 'ABORTED'):\n",
    "            print('Task (%s) failed, check it out' \\\n",
    "                  % (t_id))\n",
    "            flag['taskRunning'] = False\n",
    "        else:\n",
    "            sleep(loop_time) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run _second-stage_ task\n",
    "This is similar to our approach with the front-end task. We first get all the files in the project (there will be more since out front-end tasks have completed<sup>2</sup>). Then we seach for the file extenstion (would be more elegant here to query the outputs of the tasks).\n",
    "     \n",
    "<sup>2</sup> Note this means we _ignore_ any failed tasked. You can choose your own adventure here, e.g. re-running  (or QC-ing) failed tasks, all-or-none processing, etc.\n",
    "\n",
    "#### PROTIPS\n",
    "* Detailed documentation of this particular REST architectural style request is available [here](http://docs.cancergenomicscloud.org/docs/create-a-new-task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recheck the files in the project.\n",
    "my_files = API(path='files', query={'project': my_project.id})\n",
    "\n",
    "gtf_ind = None\n",
    "fasta_ind = None\n",
    "abund_ind = [None]\n",
    "\n",
    "for ii,f_name in enumerate(my_files.name):\n",
    "    if f_name[-len(abund_ext):] == abund_ext:                     # candidate input file\n",
    "        abund_ind.append(ii)\n",
    "    elif f_name[-len(annote_ext):] == annote_ext:\n",
    "        gtf_ind = ii\n",
    "    elif f_name[-len(ref_ext):] == ref_ext:\n",
    "        fasta_ind = ii\n",
    "abund_ind.pop(0)\n",
    "\n",
    "# 7) Format the JSON to pass values to the BACK-END workflow [for backend RNA-seq cuffquant]\n",
    "new_task = {'description': 'Quantify RNA expression using cuffdiff. created with thyroid.py',\n",
    "'name': 'second_stage',\n",
    "'app': my_inputs_second.id,                                         # little hard-coding\n",
    "'project': my_project.id,\n",
    "'inputs': {\n",
    "    'Reference':{                                               # (.fasta file)\n",
    "        'class':'File',\n",
    "        'path': my_files.id[fasta_ind],\n",
    "        'name': my_files.name[fasta_ind]\n",
    "    },\n",
    "    'Annotations': {                                            # (.gtf file)\n",
    "        'class': 'File',\n",
    "        'path': my_files.id[gtf_ind],\n",
    "        'name': my_files.name[gtf_ind]\n",
    "    },\n",
    "    'sample_files': [\n",
    "        {                                                       # .processingLists will be added later\n",
    "        'class': 'File',\n",
    "        'path': 'not_real',\n",
    "        'name': 'not_real'\n",
    "        }\n",
    "    ],\n",
    "    # Define App Settings (these are the things we \"unlocked\")\n",
    "    'FDR': 0.05,\n",
    "    'library_type': u'ff-unstranded',\n",
    "    'min_reps_for_js_test': 3,\n",
    "    'library_norm_method': u'classic-fpkm',\n",
    "    'dispersion_method': u'per-condition'\n",
    "    }\n",
    "}\n",
    "\n",
    "for ii in abund_ind:\n",
    "    file_desc = {'class': 'File',\n",
    "                'path': my_files.id[ii],\n",
    "                'name': my_files.name[ii]\n",
    "                }\n",
    "    new_task['inputs']['sample_files'].append(file_desc)\n",
    "new_task['inputs']['sample_files'].pop(0)\n",
    "\n",
    "my_task = api_call(method='POST', data=new_task, path='tasks/', query={'action': 'run'})        # task created and run\n",
    "\n",
    "print(\"You've got tasks, yaaaaayy!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
